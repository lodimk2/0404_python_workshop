{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoqyxbNaWAZNRWQpuZgsHL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lodimk2/0404_python_workshop/blob/main/Python_USB_Workshop_ANSWERKEY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_Ovl10eH6d_u"
      },
      "outputs": [],
      "source": [
        "# USB Python Workshop\n",
        "# Unsupervised Machine Learning Principles\n",
        "# scRNAseq Basic Analysis Principles\n",
        "# K Means Clustering Implementation\n",
        "# Presented by: Musaddiq Lodi\n",
        "\n",
        "# K Means Tutorial Based on: https://medium.com/@avijit.bhattacharjee1996/implementing-k-means-clustering-from-scratch-in-python-a277c23563ac\n",
        "# Also Based on https://towardsdatascience.com/create-your-own-k-means-clustering-algorithm-in-python-d7d4c9077670\n",
        "\n",
        "# GOAL: Implement K Means Clustering algorithm on scRNAsequencing data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start by loading in the scRNAseq data we will be working with.\n",
        "# For ease of loading we will be using the scanpy package.\n",
        "\n",
        "!pip install scanpy\n",
        "import scanpy as sc\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usv0gMq6AlNe",
        "outputId": "a3dc2f1d-3f40-42a3-de9f-f16e51b11d5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anndata>=0.8 (from scanpy)\n",
            "  Downloading anndata-0.10.6-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.1/122.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.9.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.3.2)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.2.1)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.25.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.0)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (2.0.3)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.11.4)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.1)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.2)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n",
            "  Downloading array_api_compat-1.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->scanpy) (1.16.0)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.10.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8026 sha256=f4bfb37cbc2ccd22379eac3c5f4022e352038cd9c205814190a93c6ffddaa92b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: array-api-compat, stdlib_list, legacy-api-wrap, session-info, pynndescent, anndata, umap-learn, scanpy\n",
            "Successfully installed anndata-0.10.6 array-api-compat-1.6 legacy-api-wrap-1.4 pynndescent-0.5.12 scanpy-1.10.0 session-info-1.0.0 stdlib_list-0.10.0 umap-learn-0.5.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Now, we will read in the input file of the count matrix as a pandas dataframe\n",
        "github_url = \"https://raw.githubusercontent.com/lodimk2/0404_python_workshop/main/workshop_scdata.csv\"\n",
        "\n",
        "# Read in the file as a pandas dataframe. We know that the first column should be row names, hence the index_col parameter being set.\n",
        "data = pd.read_csv(github_url, index_col = 0)\n"
      ],
      "metadata": {
        "id": "13gdDpFu8ywV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets inspect our data\n",
        "\n",
        "# View top 5 rows and columns\n",
        "print(data.iloc[:5, :5])\n",
        "\n",
        "\n",
        "# Check the number of rows and columns in the data\n",
        "print(\"Number of rows in dataframe:\", data.shape[0])\n",
        "print(\"Number of columns in dataframe:\", data.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRCo103j99vk",
        "outputId": "7e5a155f-3f9b-47c2-e473-b6afa1089ac1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         AGGGAACGA-GATTGCGA  TGAAGGATTCA-GGTTTCTC  ACTCCGCAT-GTTAACCA  \\\n",
            "Foxo6                     0                     0                   0   \n",
            "Trip4                     0                     0                   2   \n",
            "Slc15a3                   0                     0                   0   \n",
            "Papss1                    0                     0                   1   \n",
            "Gm13031                   0                     0                   0   \n",
            "\n",
            "         GAGCGGTA-CTTAGGTA  TGATCGACACC-ACTAGGAT  \n",
            "Foxo6                    0                     0  \n",
            "Trip4                    0                     1  \n",
            "Slc15a3                  0                     0  \n",
            "Papss1                   0                     1  \n",
            "Gm13031                  0                     0  \n",
            "Number of rows in dataframe: 500\n",
            "Number of columns in dataframe: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we will create a scanpy object from the dataframe.\n",
        "# Using a scanpy object will allow for basic scRNAseq tasks, such as:\n",
        "# 1) Normalizing the data\n",
        "# 2) Selecting highly variable features\n",
        "# 3) Clustering and visualizing\n",
        "\n",
        "# Let's walk through some basic preprocessing and quality control steps common to scRNAsequencing data.\n",
        "\n",
        "# Common convention is to call a scanpy object \"adata\"\n",
        "\n",
        "adata = sc.AnnData(data)\n",
        "print(adata)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uVZbodA-3bQ",
        "outputId": "2cbaa892-87ca-4eff-8b48-95c2268995af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnnData object with n_obs × n_vars = 500 × 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PAUSE!! Let's take a second to check our data and make sure that it makes sense. What should the observations be and what should the variables be?\n",
        "\n",
        "\n",
        "# Let's remake the scanpy object with the correct dimensions. Scanpy objects are created from matrices where the genes are the columns, and the cells are the rows.\n",
        "\n",
        "# How can we remake the object?\n",
        "\n",
        "adata = sc.AnnData(data.T)\n",
        "print(adata)\n",
        "\n",
        "# Always remember to INSPECT your data!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftQ3bw03Gv3r",
        "outputId": "136df368-b842-4283-9cfb-76939b472a68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnnData object with n_obs × n_vars = 100 × 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that the scanpy object has the correct dimensions, we can continue with some preprocessing steps.\n",
        "\n",
        "# Optionally, you can also filter out bad genes\n",
        "sc.pp.filter_genes(adata, min_cells=3)  # Remove genes detected in less than 3 cells\n",
        "\n",
        "# Perform additional preprocessing steps as needed, such as normalization, scaling, etc.\n",
        "sc.pp.normalize_total(adata, target_sum=1e4)  # Normalize total counts per cell\n",
        "sc.pp.log1p(adata)  # Log-transform the data\n",
        "\n"
      ],
      "metadata": {
        "id": "vBAX4HtpG7tL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to have an accurate comparison to perform the downstream clustering, we need the normalized and scaled data.\n",
        "\n",
        "# Access the normalized and scaled data\n",
        "log_transformed_matrix = adata.X\n",
        "\n",
        "print(adata.X)\n",
        "\n",
        "# Inspect it\n",
        "\n",
        "print(log_transformed_matrix[:5, :5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPF26RasIX9h",
        "outputId": "fdb998ea-4fa3-4cb4-f981-cc0ed0ee858f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.        0.        0.        ... 0.        0.        0.       ]\n",
            " [0.        0.        0.        ... 0.        0.        0.       ]\n",
            " [0.        4.2480025 3.569047  ... 0.        0.        3.569047 ]\n",
            " ...\n",
            " [0.        0.        0.        ... 0.        0.        0.       ]\n",
            " [0.        0.        0.        ... 0.        0.        0.       ]\n",
            " [0.        0.        0.        ... 0.        0.        0.       ]]\n",
            "[[0.        0.        0.        0.        0.       ]\n",
            " [0.        0.        0.        0.        4.091399 ]\n",
            " [0.        4.2480025 3.569047  0.        5.3370404]\n",
            " [0.        0.        0.        0.        4.1327586]\n",
            " [0.        3.7574956 3.7574956 3.7574956 4.4389033]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will work on the K-Means clustering algorithm. The K-Means clustering algorithm may be broadly summarized into 5 steps:\n",
        "# 1) Choose the number of clusters, k, that you want to create.\n",
        "# 2) Initialize k cluster centroids randomly.\n",
        "# 3) Assign each data point to the nearest centroid, creating k clusters.\n",
        "# 4) Recalculate the centroids as the mean of all data points in each cluster.\n",
        "# 5) Repeat steps 3 and 4 until convergence (centroids no longer change significantly) or for a specified number of iterations."
      ],
      "metadata": {
        "id": "Lk7PWgZzQWzJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class KMeans:\n",
        "    def __init__(self, n_clusters, max_iters=100):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iters = max_iters\n",
        "\n",
        "    def fit(self, X):\n",
        "        # Initialize centroids randomly\n",
        "        self.centroids = self._initialize_centroids(X)\n",
        "\n",
        "        for _ in range(self.max_iters):\n",
        "            # Assign each data point to the nearest centroid\n",
        "            labels = self._assign_labels(X)\n",
        "\n",
        "            # Update centroids\n",
        "            new_centroids = self._update_centroids(X, labels)\n",
        "\n",
        "            # Check for convergence\n",
        "            if np.array_equal(self.centroids, new_centroids):\n",
        "                break\n",
        "\n",
        "            self.centroids = new_centroids\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        # Randomly select initial centroids from data points\n",
        "        indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)\n",
        "        return X[indices]\n",
        "\n",
        "    def _assign_labels(self, X):\n",
        "        labels = []\n",
        "        for point in X:\n",
        "            # Compute distances from the point to centroids\n",
        "            distances = [np.linalg.norm(point - centroid) for centroid in self.centroids]\n",
        "            # Assign label based on the nearest centroid\n",
        "            labels.append(np.argmin(distances))\n",
        "        return np.array(labels)\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        new_centroids = []\n",
        "        for i in range(self.n_clusters):\n",
        "            # Extract data points assigned to the current cluster\n",
        "            cluster_points = X[labels == i]\n",
        "            # Compute the mean of data points to get the new centroid\n",
        "            new_centroid = np.mean(cluster_points, axis=0)\n",
        "            new_centroids.append(new_centroid)\n",
        "        return np.array(new_centroids)\n"
      ],
      "metadata": {
        "id": "A3D3-ssHSLl1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, lets test the clusters that we made\n",
        "\n",
        "# Create a K-Means instance with 10 clusters\n",
        "kmeans = KMeans(n_clusters=10)\n",
        "\n",
        "# Fit the model to the data\n",
        "kmeans.fit(log_transformed_matrix)\n",
        "\n",
        "# Get cluster assignments for each data point\n",
        "labels = kmeans._assign_labels(log_transformed_matrix)"
      ],
      "metadata": {
        "id": "s6CiPKrZrHQ5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHuDNGbvrmFD",
        "outputId": "31289eab-d925-4018-fe41-9162c35eb44b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 4 4 2 6 0 3 1 0 0 4 2 2 9 4 4 6 1 0 0 5 3 1 2 1 4 8 3 1 9 0 0 3 4 1 1 4\n",
            " 3 6 7 4 2 1 1 0 0 0 3 2 3 3 0 1 2 3 8 1 3 1 4 1 3 4 6 1 0 9 1 1 0 1 3 3 6\n",
            " 0 0 0 0 7 3 0 6 0 0 0 4 8 0 6 1 0 1 1 6 0 1 0 0 3 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# There we go! We have now performed clustering on our dataset, and obtained the predicted labels.\n",
        "# There are several other steps we can go from here to obtain biologically relevant conclusions related to cell states."
      ],
      "metadata": {
        "id": "yXql6BAr5ula"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}